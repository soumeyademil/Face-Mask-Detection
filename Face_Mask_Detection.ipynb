{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Face_Mask_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QtqX5YPBzmjd",
        "3wsMeheJzc5N"
      ],
      "mount_file_id": "1fho0t7grSbDjRPZx4vDRs8_sPeW5Hau2",
      "authorship_tag": "ABX9TyOUdUDb8BZdZqAZqfvH6m4E",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soumeyademil/Face-Mask-Detection/blob/main/Face_Mask_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaIH6h-dwQQ2"
      },
      "source": [
        "# Face-Mask-Detection\n",
        "### Fait par : DEMIL et GHAOUAT\n",
        "### Groupe : 2SIQ 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtqX5YPBzmjd"
      },
      "source": [
        "# Modèle : Preprocessing + Training + Tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx0w2kqdUTra"
      },
      "source": [
        "## Préparation des données\n",
        "\n",
        "*   Le dataset est téléchargé à partir de Kaggle https://www.kaggle.com/omkargurav/face-mask-dataset.\n",
        "*   On utilise drive pour stocker les images.\n",
        "*   On divisera ensuite les données sur 3 repertoires : train_dir (80%), validation_dir (10%), test_dir (10%).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "qTVb-CR6kANW",
        "outputId": "2f4a896f-9a48-4db6-857e-91ee1ddde7ce"
      },
      "source": [
        "#Configuration environment\n",
        "'''\n",
        "import os\n",
        "\n",
        "os.environ['KAGGLE_USERNAME'] = \"soumeyademil\" # username from the json file\n",
        "os.environ['KAGGLE_KEY'] = \"61ee9cfa1c91fb28180125ffa84acea4\" # key from the json file\n",
        "\n",
        "!kaggle datasets download -d omkargurav/face-mask-dataset\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/face-mask-dataset.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/drive/MyDrive/Kaggle/face_mask_data\")\n",
        "\n",
        "'''    "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport os\\n\\nos.environ[\\'KAGGLE_USERNAME\\'] = \"soumeyademil\" # username from the json file\\nos.environ[\\'KAGGLE_KEY\\'] = \"61ee9cfa1c91fb28180125ffa84acea4\" # key from the json file\\n\\n!kaggle datasets download -d omkargurav/face-mask-dataset\\n\\nimport zipfile\\nwith zipfile.ZipFile(\"/content/face-mask-dataset.zip\", \\'r\\') as zip_ref:\\n    zip_ref.extractall(\"/content/drive/MyDrive/Kaggle/face_mask_data\")\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "KFOKnPHvoelJ",
        "outputId": "a930cd3f-7651-42b1-8e73-bdab663209ff"
      },
      "source": [
        "'''\n",
        "import splitfolders\n",
        "input_folder='/content/drive/MyDrive/Kaggle/face_mask_data'\n",
        "output='/content/drive/MyDrive/Kaggle/face_mask_data_processed'\n",
        "splitfolders.ratio(input_folder, output, seed=42, ratio=(.8, .1, .1))\n",
        "'''"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nimport splitfolders\\ninput_folder='/content/drive/MyDrive/Kaggle/face_mask_data'\\noutput='/content/drive/MyDrive/Kaggle/face_mask_data_processed'\\nsplitfolders.ratio(input_folder, output, seed=42, ratio=(.8, .1, .1))\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrF9uDhl0h0k"
      },
      "source": [
        "base_path = '/content/drive/MyDrive/Kaggle/face_mask_data_processed/'\n",
        "\n",
        "PATH_TRAIN_MASK = base_path + 'train/with_mask/'\n",
        "PATH_VAL_MASK = base_path + 'val/with_mask/'\n",
        "PATH_TEST_MASK = base_path + 'test/with_mask/'\n",
        "\n",
        "PATH_TRAIN_NO_MASK = base_path + 'train/without_mask/'\n",
        "PATH_VAL_NO_MASK = base_path + 'val/without_mask/'\n",
        "PATH_TEST_NO_MASK = base_path + 'test/without_mask/'\n",
        "\n",
        "PATH_TRAIN = base_path + 'train/'\n",
        "PATH_VAL = base_path + 'val/'\n",
        "PATH_TEST = base_path + 'test/'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UBHospsWC01"
      },
      "source": [
        "Dans cette cellule on determine quelques paramètres et on normalise les données, pour ensuite créer des objets ImageDataGenerator qui vont permettre au modèle de récupérer les images à partir des repertoires directement.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVzx7-OJjdjt",
        "outputId": "ce27deda-92af-49c2-f149-a35e26a7d3ae"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "batch_size = 32\n",
        "target_size = (200, 200)\n",
        "num_classes = 2\n",
        "num_epochs = 10\n",
        "\n",
        "\n",
        "datagen_train = ImageDataGenerator(rescale=1/255)\n",
        "datagen_val = ImageDataGenerator(rescale=1/255)\n",
        "datagen_test = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "train_generator = datagen_train.flow_from_directory(PATH_TRAIN,\n",
        "                                                    target_size=target_size,\n",
        "                                                    color_mode='rgb',\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='binary')\n",
        "\n",
        "val_generator = datagen_val.flow_from_directory(PATH_VAL,\n",
        "                                                    target_size=target_size,\n",
        "                                                    color_mode='rgb',\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='binary')\n",
        "\n",
        "test_generator = datagen_test.flow_from_directory(PATH_TEST,\n",
        "                                                    target_size=target_size,\n",
        "                                                    color_mode='rgb',\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='binary',\n",
        "                                                    shuffle=False)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6042 images belonging to 2 classes.\n",
            "Found 754 images belonging to 2 classes.\n",
            "Found 757 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DqPEfICDW3c",
        "outputId": "dd7c02ff-89a3-4862-fa26-429d671358b1"
      },
      "source": [
        "train_generator.class_indices"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'with_mask': 0, 'without_mask': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnpiJYi_XGLH"
      },
      "source": [
        "Pour le modèle, on utilisera le transfert learning, avec le modèle ResNet50 qui est un CNN qui a été entrainé sur un million d'images. On lui ajoutera une couche dense de 10 unités cachées, et une couche de sortie avec une unité utilisant la fonction d'activation sigmoid qui est la plus adapté pour la classification binaire."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rMm2pf6MER_"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRFDZeWgeHcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aa4ab05-ff3c-4f0d-d076-a6923f30a2b4"
      },
      "source": [
        "md = ResNet50(include_top=False, input_shape=(200, 200, 3))\n",
        "input_layer = Input(shape=(200, 200, 3), name = 'input')\n",
        "layer1 = md(input_layer)\n",
        "layer2 = Flatten()(layer1)\n",
        "layer3 = Dense(10, activation='relu')(layer2)\n",
        "output_layer = Dense(1, activation='sigmoid')(layer3)\n",
        "\n",
        "model = Model(inputs = [input_layer], outputs = output_layer)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 0s 0us/step\n",
            "94781440/94765736 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PwzExX7h1X5"
      },
      "source": [
        "opt = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYT95CGDh-ew",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eef76da5-0206-4086-e0d0-7534daf96a32"
      },
      "source": [
        "checkpoint = ModelCheckpoint(\"/content/drive/MyDrive/TP_IMN_Face_Mask/model_weights.h5\", monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "history = model.fit(train_generator, batch_size=batch_size, epochs=num_epochs, validation_data=val_generator, callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "  2/189 [..............................] - ETA: 52:59 - loss: 2.4066 - acc: 0.4219  "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCee7iY9YgNm"
      },
      "source": [
        "On a obtenu une précision de 94% sur le validation set, les paramètres correspondant à cette valeur(max) sont enregistrés dans un fichier.h5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POMexApejaYg"
      },
      "source": [
        "model_json = model.to_json()\n",
        "with open(\"/content/drive/MyDrive/TP_IMN_Face_Mask/model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naDrQXbVY3ri"
      },
      "source": [
        "La matrice de confusion permet de visualiser la qualité de classification du modèle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv7LxYnPMGX1"
      },
      "source": [
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = load_model('/content/drive/MyDrive/TP_IMN_Face_Mask/model_weights.h5')\n",
        "predictions = model.predict_generator(test_generator)\n",
        "y_pred = (predictions > 0.5)\n",
        "y_test = test_generator.classes\n",
        "class_names = test_generator.class_indices.keys()\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "# compute confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# plot normalized confusion matrix\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names, title='Normalized confusion matrix')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-skLRLyWie8f"
      },
      "source": [
        "loss, acc = model.evaluate(test_generator) \n",
        "print(\"Test accuracy = \" , acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XegQHYpSlH6X"
      },
      "source": [
        "from keras.models import model_from_json\n",
        "import numpy as np\n",
        "\n",
        "class FaceMaskModel(object):\n",
        "\n",
        "    CLASSES_LIST = [\"WITH MASK\",\n",
        "                    \"WITHOUT MASK\"]\n",
        "\n",
        "    def __init__(self, model_json_file, model_weights_file):\n",
        "        # load model from JSON file\n",
        "        with open(model_json_file, \"r\") as json_file:\n",
        "            loaded_model_json = json_file.read()\n",
        "            self.loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "        # load weights into the new model\n",
        "        self.loaded_model.load_weights(model_weights_file)\n",
        "        #self.loaded_model._make_predict_function()\n",
        "\n",
        "    def predict_class(self, img):\n",
        "        self.preds = self.loaded_model.predict(img)\n",
        "        return FaceMaskModel.CLASSES_LIST[self.preds > 0.5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT4PZTAI0Rpb"
      },
      "source": [
        "import tensorflow\n",
        "tensorflow.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wsMeheJzc5N"
      },
      "source": [
        "# Reconnaissance en temps reel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEmkUPJOZVoc"
      },
      "source": [
        "Dans cette partie, on utilise le framework Flask qui va permettre de créer une interface graphique web pour notre application."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwDX8jKfzcAb"
      },
      "source": [
        "from tensorflow.keras.models import model_from_json\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFneZNPMzyZi"
      },
      "source": [
        "CLASSES_LIST = [\"Avec masque\",\n",
        "                \"Sans masque!\"]\n",
        "\n",
        "beep_duration = 1000  \n",
        "beep_freq = 440  \n",
        "\n",
        "facec = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "model = load_model(\"Face_Mask_Model//model_weights.h5\")\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "\n",
        "class VideoCamera(object):\n",
        "    def __init__(self):\n",
        "        self.video = cv2.VideoCapture(0)\n",
        "\n",
        "    def __del__(self):\n",
        "        self.video.release()\n",
        "\n",
        "    # returns camera frames along with bounding boxes and predictions\n",
        "\n",
        "    def get_frame(self):\n",
        "        _, fr = self.video.read()\n",
        "        faces = facec.detectMultiScale(fr)\n",
        "\n",
        "        for (x, y, w, h) in faces:\n",
        "            fc = fr[y:y+h, x:x+w]\n",
        "\n",
        "            roi = cv2.resize(fc, (200, 200))\n",
        "            roi = roi / 255.\n",
        "            pred = model.predict(roi[np.newaxis, :, :, :])\n",
        "            cond = (pred < 0.5)\n",
        "            text = CLASSES_LIST[int(cond)]\n",
        "            \n",
        "            if (cond) :\n",
        "                cv2.putText(fr, text, (x, y), font, 1, (0, 255, 255), 2)\n",
        "                cv2.rectangle(fr,(x,y),(x+w,y+h),(0,0,255),2)\n",
        "                winsound.Beep(beep_freq, beep_duration)\n",
        "            else:\n",
        "                cv2.putText(fr, text, (x, y), font, 1, (255, 255, 0), 2)\n",
        "                cv2.rectangle(fr,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "\n",
        "        _, jpeg = cv2.imencode('.jpg', fr)\n",
        "        return jpeg.tobytes()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drH3ZxCnz53D"
      },
      "source": [
        "import camera\n",
        "from flask import Flask, render_template, Response\n",
        "import os\n",
        "#from camera import VideoCamera\n",
        "\n",
        "app = Flask(__name__, template_folder='template')\n",
        "\n",
        "#os.environ['FLASK_APP'] = app\n",
        "os.environ['FLASK_ENV'] = 'development'\n",
        "\n",
        "\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "\n",
        "def gen(camera):\n",
        "    while True:\n",
        "        frame = camera.get_frame()\n",
        "        yield (b'--frame\\r\\n'\n",
        "               b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame + b'\\r\\n\\r\\n')\n",
        "\n",
        "@app.route('/video_feed')\n",
        "def video_feed():\n",
        "    return Response(gen(VideoCamera()),\n",
        "                    mimetype='multipart/x-mixed-replace; boundary=frame')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    from werkzeug.serving import run_simple\n",
        "    run_simple('localhost', 9000, app)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}